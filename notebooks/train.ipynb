{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/javierj/miniconda3/envs/gatherer-sage/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['messages'],\n",
       "     num_rows: 1960\n",
       " }),\n",
       " {'messages': [{'content': 'Using the information contained in the context,\\ngive a comprehensive and concise answer to the question.\\nRespond only to the question asked, response should be concise and relevant to the question.\\nProvide the number of the rule when relevant.\\nIf the answer cannot be deduced from the context, do not give an answer.\\nThe questions are related with Magic The Gathering card game.',\n",
       "    'role': 'system'},\n",
       "   {'content': \"Context:\\n\\nExtracted documents:\\nDocument 0:::\\nName: Dermoplasm\\nMana Cost: 2 colorless, blue\\nType: Creature — Shapeshifter\\nText: Flying\\nMorph 2 colorless, blue, blue,                                         2 colorless 2 colorless              3 colorless. Turn it face up any time for its morph cost.)\\nWhen Dermoplasm is turned face up, you may put a creature card with a morph ability from your hand onto the battlefield face up. If you do, return Dermoplasm to its owner's hand.\\nStats: 1 power, 1 toughness\\nRules:\\n1. The trigger occurs when you use the Morph ability to turn the card face up, or when an effect turns it face up. It will not trigger on being revealed or on leaving the battlefield.\\n\\nLegal in: commander, duel, legacy, oathbreaker, penny, predh, premodern, vintageDocument 1:::\\n14. Unlike a face-down creature that was cast using the morph ability, a manifested creature may still be turned face up after it loses its abilities if it’s a creature card.\\n15. You must ensure that your face-down spells and permanents can easily be differentiated from each other. You’re not allowed to mix up the cards that represent them on the battlefield in order to confuse other players. The order they entered the battlefield should remain clear. Common methods for indicating this include using markers or dice, or simply placing them in order on the battlefield. You must also track how each became face down (manifested, cast face down using the morph ability, and so on).\\n16. You’ll still manifest the top card of your library even if the “Form” isn’t on the battlefield as its enters-the-battlefield ability resolves.\\n17. Any time you have priority, you may turn a manifested creature face up by revealing that it’s a creature card (ignoring any type-changing effects that might be applying to it) and paying its mana cost. This is a special action. It doesn’t use the stack and can’t be responded to.\\n18. At any time, you can look at a face-down permanent you control. You can’t look at face-down permanents you don’t control unless an effect instructs you to do so.\\n19. Because face-down creatures don’t have a name, they can’t have the same name as any other creature or share any creature types with any other creature, even another face-down creature.\\n20. If an effect tries to return a face-down creature to the battlefield after it leaves (such as Aminatou’s second ability or Adarkar Valkyrie’s delayed triggered ability), that effect returns the card face up. If it tries to put an instant or sorcery card onto the battlefield this way, that card remains in its current zone instead.Document 2:::\\nRules:\\n1. Are you still reading? That’s awesome. Face-down cards are tricky. One more ruling to go.\\n2. Because face-down creatures don’t have a name, they can’t have the same name as any other creature or share any creature types with any other creature, even another face-down creature.\\n3. If a card with spellmorph is manifested, you can cast it from the battlefield.\\n4. If a face-down creature with spellmorph loses all abilities, it can’t be cast from the battlefield.\\n5. If a face-down permanent you control leaves the battlefield, you must reveal it. If a face-down spell you control leaves the stack other than by resolving, you must reveal it. You must also reveal all face-down spells and permanents you control if you leave the game or if the game ends. The game doesn’t count them as being turned face up when you reveal them this way.\\n6. If an effect tries to return a face-down creature to the battlefield after it leaves (such Momentary Blink or Adarkar Valkyrie’s delayed triggered ability), that effect returns the card face up. If it tries to put an instant or sorcery card onto the battlefield this way, that card remains in its current zone instead.\\n7. Spellmorph is a variant of morph. A spellmorph ability is a morph ability, and a spellmorph cost is a morph cost. All rules and rulings for morph apply to spellmorph, except for how to turn it face up.\\n8. The controller of the face-down creature with spellmorph can cast it, regardless of who cast the face-down creature spell.\\n9. The face-down characteristics of a permanent are copiable values. If another object becomes a copy of a face-down creature or if a token is created that’s a copy of a face-down creature, that new object is a 2/2 colorless face-up creature with no abilities.Document 3:::\\nspell with the same characteristics), and pay {3} rather than pay its mana cost. This follows the \\nrules for paying alternative costs. You can use a morph ability to cast a card from any zone from \\nwhich you could normally cast  it. When the spell resolves, it enters the battlefield with the same \\ncharacteristics the spell had. The morph effect applies to the face- down object wherever it is, \\nand it ends when the permane nt is turned face up.  \\n \\n702.37d  You can ’t normally cast a card face down . A morph  ability  allows you to do so.  \\n \\n702.37e  Any time  you have priority, you may turn a face -down permanent you control  with a \\nmorph ability  face up. This is a special action; it doesn ’t use the stack (see rule 116 ). To do this, \\nshow all players what the permanent ’s morph cost would be if it were face up, pay that cost, \\nthen turn the permanent face up. (If the permanent wouldn ’t have a morph cost if it were face \\nup, it can ’t be turned face up this way.) The morph effect on it ends, and it regains its normal \\ncharacteristics. Any abilities relating to the permanent entering the battlefield don ’t trigger when \\nit’s turned face up and don’ t have any  effect, because the permanent has already entered the \\nbattlefield.  \\n \\n702.37f If a permanent ’s morph cost includes X, other abilities of that permanent may also refer to \\nX. The value of X in those abilities is equal to the value of X chosen as the morph special action was taken.  \\n \\n702.37g See rule 708 , “Face- Down Spells and Permanents, ” for more information about how to cast \\ncards with a morph ability.  \\n \\n702.38. Amplify  \\n \\n702.38a Amplify is a static ability. “ Amplify N ” means “As this object enters the battlefield, revealDocument 4:::\\n7. If a face-down permanent leaves the battlefield, you must reveal it. You must also reveal all face-down spells and permanents you control if you leave the game or if the game ends.\\n8. Morph lets you cast a card face down by paying {3}, and lets you turn the face-down permanent face up any time you have priority by paying its morph cost.\\n9. Once Ashcloud Phoenix returns to the battlefield face down, each player will know which face-down creature it is. You can't mix up the positions of your face-down permanents to disguise this.\\n10. The face-down spell has no mana cost and has a mana value of 0. When you cast a face-down spell, put it on the stack face down so no other player knows what it is, and pay {3}. This is an alternative cost.\\n11. When the spell resolves, it enters the battlefield as a 2/2 creature with no name, mana cost, creature types, or abilities. It's colorless and has a mana value of 0. Other effects that apply to the creature can still grant it any of these characteristics.\\n12. You must ensure that your face-down spells and permanents can easily be differentiated from each other. You're not allowed to mix up the cards that represent them on the battlefield in order to confuse other players. The order they entered the battlefield should remain clear. Common methods for doing this include using markers or dice, or simply placing them in order on the battlefield.\\n---\\nNow here is the question you need to answer.\\n\\nQuestion: Morph returned to hand\\nSo I had a morph in the graveyard. If I return it to my hand then play a morph face down. Should my opponents know if it was the morph that just got returned from the graveyard?\\n\\nAlso does this apply to bouncing a face up morph to hand\",\n",
       "    'role': 'user'},\n",
       "   {'content': \"Answer: You had it on the face-down Battlefield. It died, and as soon as it left the Battlefield, it was revealed. And, face-up in the Graveyard.\\n\\nThen, you moved it from your Graveyard (public) to your Hand (hidden). \\n\\nUnless there were no other cards in your Hand, your opponent won't know whether the card you're currently casting face-down is the same card you just returned or a different card... But, they may make an educated guess.\",\n",
       "    'role': 'assistant'}]})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dataset_gen(data):\n",
    "    for idx, row in data.iterrows():\n",
    "\n",
    "        prompt = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"\"\"Using the information contained in the context,\n",
    "give a comprehensive and concise answer to the question.\n",
    "Respond only to the question asked, response should be concise and relevant to the question.\n",
    "Provide the number of the rule when relevant.\n",
    "If the answer cannot be deduced from the context, do not give an answer.\n",
    "The questions are related with Magic The Gathering card game.\"\"\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"\"\"Context:\n",
    "{row['context']}\n",
    "---\n",
    "Now here is the question you need to answer.\n",
    "\n",
    "Question: {row['question']}\"\"\",\n",
    "            },\n",
    "            {\"role\": \"assistant\", \"content\": f\"Answer: {row['answer']}\"},\n",
    "        ]\n",
    "\n",
    "        yield {\"messages\": prompt}\n",
    "\n",
    "\n",
    "reddit_df = pd.read_csv(\"data/reddit/reddit_qa_dataset_with_context.csv\")\n",
    "\n",
    "dataset = Dataset.from_generator(dataset_gen, gen_kwargs={\"data\": reddit_df})\n",
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = dataset[\"train\"]\n",
    "test_dataset = dataset[\"test\"]\n",
    "\n",
    "train_dataset, train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.30it/s]\n"
     ]
    }
   ],
   "source": [
    "READER_MODEL_NAME = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(READER_MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# BitsAndBytesConfig int-4 config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    READER_MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    # attn_implementation=\"flash_attention_2\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=bnb_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1960/1960 [00:00<00:00, 2021.51 examples/s]\n",
      "Map: 100%|██████████| 490/490 [00:00<00:00, 2037.77 examples/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjavier-jimenez99\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/javierj/gatherer-sage/wandb/run-20240530_173714-rjhtc8ig</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/javier-jimenez99/huggingface/runs/rjhtc8ig' target=\"_blank\">./results_modified</a></strong> to <a href='https://wandb.ai/javier-jimenez99/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/javier-jimenez99/huggingface' target=\"_blank\">https://wandb.ai/javier-jimenez99/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/javier-jimenez99/huggingface/runs/rjhtc8ig' target=\"_blank\">https://wandb.ai/javier-jimenez99/huggingface/runs/rjhtc8ig</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/home/javierj/miniconda3/envs/gatherer-sage/lib/python3.9/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='490' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  6/490 00:35 < 1:11:54, 0.11 it/s, Epoch 0.01/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='48' max='490' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 48/490 03:01 < 28:29, 0.26 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mEl kernel se bloqueó al ejecutar código en la celda actual o en una celda anterior. \n",
      "\u001b[1;31mRevise el código de las celdas para identificar una posible causa del error. \n",
      "\u001b[1;31mHaga clic <a href='https://aka.ms/vscodeJupyterKernelCrash'>aquí</a> para obtener más información. \n",
      "\u001b[1;31mVea Jupyter <a href='command:jupyter.viewOutput'>log</a> para obtener más detalles."
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer\n",
    "import evaluate\n",
    "\n",
    "# LoRA Config\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=128,\n",
    "    lora_dropout=0.05,\n",
    "    r=256,\n",
    "    bias=\"none\",\n",
    "    target_modules=\"all-linear\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    print(pred)\n",
    "    return {\n",
    "        \"rouge1\": rouge.compute(predictions=pred[\"pred\"], references=pred[\"label\"])[\n",
    "            \"rouge1\"\n",
    "        ].mid.fmeasure,\n",
    "        \"rouge2\": rouge.compute(predictions=pred[\"pred\"], references=pred[\"label\"])[\n",
    "            \"rouge2\"\n",
    "        ].mid.fmeasure,\n",
    "        \"rougeL\": rouge.compute(predictions=pred[\"pred\"], references=pred[\"label\"])[\n",
    "            \"rougeL\"\n",
    "        ].mid.fmeasure,\n",
    "        \"bleu\": bleu.compute(predictions=pred[\"pred\"], references=pred[\"label\"])[\n",
    "            \"score\"\n",
    "        ],\n",
    "    }\n",
    "\n",
    "\n",
    "# Training Params\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./results_modified\",  # directory to save and repository id\n",
    "    num_train_epochs=1,  # number of training epochs\n",
    "    per_device_train_batch_size=1,  # batch size per device during training\n",
    "    per_device_eval_batch_size=1,  # batch size for evaluation\n",
    "    eval_accumulation_steps=4,  # number of steps before performing a backward/update pass\n",
    "    gradient_accumulation_steps=4,  # number of steps before performing a backward/update pass\n",
    "    gradient_checkpointing=True,  # use gradient checkpointing to save memory\n",
    "    optim=\"adamw_torch_fused\",  # use fused adamw optimizer\n",
    "    logging_steps=10,  # log every 10 steps\n",
    "    eval_steps=5,  # evaluate every 100 steps\n",
    "    eval_strategy=\"steps\",  # evaluate every 5 steps\n",
    "    save_strategy=\"epoch\",  # save checkpoint every epoch\n",
    "    learning_rate=2e-4,  # learning rate, based on QLoRA paper\n",
    "    bf16=True,  # use bfloat16 precision\n",
    "    tf32=True,  # use tf32 precision\n",
    "    max_grad_norm=0.3,  # max gradient norm based on QLoRA paper\n",
    "    warmup_ratio=0.03,  # warmup ratio based on QLoRA paper\n",
    "    lr_scheduler_type=\"constant\",  # use constant learning rate scheduler\n",
    "    # push_to_hub=True,                       # push model to hub\n",
    "    # report_to=\"tensorboard\",                # report metrics to tensorboard\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "max_seq_length = 3072  # max sequence length for model and packing of the dataset\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    # packing=True,\n",
    "    dataset_kwargs={\n",
    "        \"add_special_tokens\": False,  # We template with special tokens\n",
    "        \"append_concat_token\": False,  # No need to add additional separator token\n",
    "    },\n",
    ")\n",
    "\n",
    "# Training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model\n",
    "trainer.model.save_pretrained(\"model/gatherer_sage_model/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gatherer-sage",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
