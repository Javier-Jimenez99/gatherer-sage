{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/javierj/miniconda3/envs/gatherer-sage/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  1.90it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  1.92it/s]\n",
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'OlmoForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline,\n",
    ")\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "# BitsAndBytesConfig int-4 config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "base_model_path = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "adapter_path = \"model/mistral-gatherer-sage-v1/4vgx59fr/best_model\"\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_path,\n",
    "    device_map=\"auto\",\n",
    "    # attn_implementation=\"flash_attention_2\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "\n",
    "model_peft = PeftModel.from_pretrained(model, adapter_path)\n",
    "# model_peft = model_peft.merge_and_unload()\n",
    "\n",
    "model_peft2 = AutoModelForCausalLM.from_pretrained(\n",
    "    adapter_path,\n",
    "    device_map=\"auto\",\n",
    "    # attn_implementation=\"flash_attention_2\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "llm_pipeline_no_peft = pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    do_sample=True,\n",
    "    temperature=0.2,\n",
    "    repetition_penalty=1.1,\n",
    "    return_full_text=False,\n",
    "    max_new_tokens=500,\n",
    ")\n",
    "\n",
    "llm_pipeline_peft = pipeline(\n",
    "    model=model_peft,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    do_sample=True,\n",
    "    temperature=0.2,\n",
    "    repetition_penalty=1.1,\n",
    "    return_full_text=False,\n",
    "    max_new_tokens=500,\n",
    ")\n",
    "\n",
    "llm_pipeline_peft2 = pipeline(\n",
    "    model=model_peft2,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    do_sample=True,\n",
    "    temperature=0.2,\n",
    "    repetition_penalty=1.1,\n",
    "    return_full_text=False,\n",
    "    max_new_tokens=500,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gatherer_sage.rag import RAG\n",
    "\n",
    "rag = RAG(vector_database_path=\"data/rag_vector_db\")\n",
    "\n",
    "prompt_in_chat_format = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"\"\"Using the information contained in the context,\n",
    "give a comprehensive and concise answer to the question.\n",
    "Respond only to the question asked, response should be concise and relevant to the question.\n",
    "Provide the number of the rule when relevant.\n",
    "If the answer cannot be deduced from the context, do not give an answer.\n",
    "The questions are related with Magic The Gathering card game.\n",
    "        \n",
    "Context:\n",
    "{context}\n",
    "---\n",
    "Now here is the question you need to answer.\n",
    "\n",
    "Question: {question}\"\"\",\n",
    "    },\n",
    "]\n",
    "\n",
    "prompt_template = tokenizer.apply_chat_template(\n",
    "    prompt_in_chat_format, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "\n",
    "\n",
    "def get_answer(question, model):\n",
    "    context = rag.retrieve_context(question)\n",
    "    prompt = prompt_template.format(context=context, question=question)\n",
    "    return model(prompt)[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 22.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: No.\n",
      "\n",
      "Thoughtseize does not target anything. It simply says \"Target player\". So, it can target someone who has no cards in hand.\n",
      "\n",
      "&gt; 115.1a An object is targeted if it’s specified as the target of a spell or ability. Casting a spell or activating an ability that specifies a target is called casting a targeted spell or activating a targeted ability, respectively. See rule 601.2c.\n",
      "\n",
      "So, Thoughtseize is not a targeted spell. And, since it's not a targeted spell, it does not check whether its target is legal until it resolves.\n",
      "\n",
      "&gt; 115.1b A spell or ability is targeted if it specifies a target object or player but doesn’t specify a zone or object type. Such objects are called targeted objects.\n",
      "\n",
      "And, since it's not a targeted spell, it does not check whether its target is legal after it resolves.\n",
      "\n",
      "&gt; 115.1c A spell or ability is targeted if it specifies a target object or player and specifies a zone or object type that contains that object or player. Such objects are called targeted objects.\n",
      "\n",
      "So, even if Nico has no cards in hand, Thoughtseize will still resolve and cause them to lose 2 life.\n",
      "\n",
      "---\n",
      "\n",
      "Compare Thoughtseize to [[Sudden Death]]. Sudden Death is a targeted spell. It checks whether its target is legal before it resolves. And, it checks again after it resolves.\n",
      "\n",
      "&gt; 115.1d A spell or ability is targeted if it specifies a target object or player and specifies a zone or object type that contains that object or player. Such objects are called targeted objects.\n",
      "\n",
      "So, if Nico has no cards in hand, Sudden Death will fail to resolve because its target is illegal.\n",
      "\n",
      "---\n",
      "\n",
      "Note that there is also a third category of spells and abilities...\n",
      "\n",
      "&gt; 115.1e A spell or ability is targeted if it specifies a target object or player and specifies a characteristic about that object or player. Such objects are called targeted objects.\n",
      "\n",
      "But, these are not relevant here.\n",
      "\n",
      "---\n",
      "\n",
      "Also note that there is\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 22.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Yes.\n",
      "\n",
      "Thoughtseize does not check what happens while it is on the Stack. It checks what happens after it resolves.\n",
      "\n",
      "So, if Nico has no Cards in Hand after Thoughtseize resolves, then Allyson will lose 2 Life.\n",
      "\n",
      "---\n",
      "\n",
      "Alternatively, if Nico has no Cards in Hand before Thoughtseize resolves, then Allyson will not lose 2 Life.\n",
      "\n",
      "---\n",
      "\n",
      "Note; If Thoughtseize is countered, then nothing happens. No one loses 2 life.\n",
      "\n",
      "&gt; 701.5j If a spell or ability puts a card into a player’s hand without using the word “draw,” the player draws that card while performing any other actions required by that spell or ability. If the player is instructed to draw multiple cards, they draw them one at a time. If the player is instructed to draw cards until a certain event occurs, they continue drawing cards until that event occurs. If the player is instructed to draw cards equal to a certain number, they draw that many cards. If the player is instructed to draw cards equal to the value of a variable, they draw that many cards. If the player is instructed to draw cards equal to the value of a variable plus a certain number, they draw that many cards plus that certain number. If the player is instructed to draw cards equal to the value of a variable minus a certain number, they draw that many cards minus that certain number. If the player is instructed to draw cards equal to the value of a variable times a certain number, they draw that many cards times that certain number. If the player is instructed to draw cards equal to the value of a variable divided as a certain number, they draw that many cards divided as that certain number. If the player is instructed to draw cards equal to the value of a variable modulo a certain number, they draw that many cards modulo that certain number. If the player is instructed to draw cards equal to the value of a variable plus one, they draw that many cards plus one. If the player is instructed to draw cards equal to the value of a variable minus one, they draw that many cards minus one. If the player is instructed to draw cards equal to the value of a variable times one, they draw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 22.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Nope, Thoughtseize says \"You choose a non-Land Card\" so Nico chooses a non-Land Card, but since there are none in their Hand, they don't get to discard anything. So Allyson does not lose 2 life.\n",
      "\n",
      "&gt; 701.1 \n",
      "&gt;\n",
      "&gt; 701.1a To choose a card name, a player announces the name of a card they know is in the appropriate zone (see rule 601.2i), or they announce that they’re casting a spell with certain characteristics (such as a card with mana cost {U} or a creature with power 3) and ask whether the player being addressed knows the name of a card that matches those characteristics. If the player being addressed doesn’t know the name of such a card, they say so. If the player who chose a card name or announced characteristics doesn’t like the answer, they may repeat the process until they’re satisfied. If the player who chose a card name or announced characteristics doesn’t like the answer, they may repeat the process until they’re satisfied. If the player who chose a card name or announced characteristics doesn’t like the answer, they may repeat the process until they’re satisfied. If the player who chose a card name or announced characteristics doesn’t like the answer, they may repeat the process until they’re satisfied. If the player who chose a card name or announced characteristics doesn’t like the answer, they may repeat the process until they’re satisfied. If the player who chose a card name or announced characteristics doesn’t like the answer, they may repeat the process until they’re satisfied. If the player who chose a card name or announced characteristics doesn’t like the answer, they may repeat the process until they’re satisfied. If the player who chose a card name or announced characteristics doesn’t like the answer, they may repeat the process until they’re satisfied. If the player who chose a card name or announced characteristics doesn’t like the answer, they may repeat the process until they’re satisfied. If the player who chose a card name or announced characteristics doesn’t like the answer, they may repeat the process until they’re satisfied. If the player who chose a card name or announced characteristics doesn’t like the answer, they\n"
     ]
    }
   ],
   "source": [
    "q = \"Allyson casts Thoughtseize targeting Nico. In response, Nico casts their last card, and when Thoughtseize resolves, Nico has zero cards in hand. Does Allyson lose 2 life?\"\n",
    "print(get_answer(q, llm_pipeline_no_peft))\n",
    "print(get_answer(q, llm_pipeline_peft))\n",
    "print(get_answer(q, llm_pipeline_peft2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 18.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Yes.\n",
      "\n",
      "&gt; 702.19b Auras attached to a permanent that moves to a hidden zone (see rule 113.6) aren’t put into their owners’ graveyards. Spells and abilities that would cause an Aura to be put into its owner’s graveyard instead exile it. An Aura that’s exiled is removed from the game. See rule 800.4.\n",
      "\n",
      "So, if Grist is in a hidden zone, it's exiled. \n",
      "\n",
      "&gt; 800.4. If a card leaves the game, it leaves the game completely. It doesn’t go to a hidden zone.\n",
      "\n",
      "If Grist is in a public zone, it's not exiled. \n",
      "\n",
      "&gt; 702.19c If an effect attempts to put an Aura onto the battlefield attached to a permanent that it can’t legally enchant, the Aura remains in its current zone, unless that zone is a stack. In that case, the Aura is put into its owner’s graveyard instead of entering the battlefield. If the Aura is a token, it isn’t created.\n",
      "\n",
      "If Grist is in a public zone, and there's a legal permanent for it to attach to, it attaches to that permanent.\n",
      "\n",
      "&gt; 702.19d If an effect allows a player to put an Aura onto the battlefield attached to a permanent that it can’t legally enchant, the player must choose a permanent that is either a) represented by a card in the player’s hand, b) represented by a card in the player’s library, or c) a token. The Aura enters the battlefield attached to that permanent. If the Aura is a token, it isn’t created. If the chosen permanent is a token, the Aura enters the battlefield attached to that token’s controller rather than the token itself. If the chosen permanent is represented by a card in the player’s hand or library, the Aura enters the battlefield attached to that card’s controller rather than the card itself. If the Aura is a token, it isn’t created. If the chosen permanent is represented by a card in the player’s hand\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 19.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: No.\n",
      "\n",
      "Chord of Calling is a Spell. It is not a Creature Card. So, it does not trigger Grist's +1 ability.\n",
      "\n",
      "And, since Grist's +1 ability did not trigger, there is nothing to repeat the process. So, Grist remains in Exile.\n",
      "\n",
      "---\n",
      "\n",
      "Alternatively, if they had cast [[Bolt Bend]], which is a Creature Card, then it would enter the Battlefield as a 1/1 Insect Creature. And, since it is a Creature Card, it would trigger Grist's +1 ability.\n",
      "\n",
      "So, they would get a token and mill a card. And, since they milled a Creature Card, they would repeat the process.\n",
      "\n",
      "And, since Grist is not on the Battlefield, it would remain in Exile.\n",
      "\n",
      "---\n",
      "\n",
      "Or, they could have cast [[Rishadan Brigand]], which is a Creature Card. It would enter the Battlefield as a 1/1 Insect Creature. And, since it is a Creature Card, it would trigger Grist's +1 ability.\n",
      "\n",
      "So, they would get a token and mill a card. And, since they milled a Creature Card, they would repeat the process.\n",
      "\n",
      "And, since Grist is not on the Battlefield, it would remain in Exile.\n",
      "\n",
      "---\n",
      "\n",
      "Or, they could have cast [[Cavalier of Night]] which is a Creature Card. It would enter the Battlefield as a 1/1 Insect Creature. And, since it is a Creature Card, it would trigger Grist's +1 ability.\n",
      "\n",
      "So, they would get a token and mill a card. And, since they milled a Creature Card, they would repeat the process.\n",
      "\n",
      "And, since Grist is not on the Battlefield, it would remain in Exile.\n",
      "\n",
      "---\n",
      "\n",
      "Or, they could have cast [[Nightmare Shepherd]], which is a Creature Card. It would enter the Battlefield as a 1/1 Insect Creature. And, since it is a Creature Card, it would trigger Grist's +1 ability.\n",
      "\n",
      "So, they would get a token and mill a card. And,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 19.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: No.\n",
      "\n",
      "Aubrey puts Grist onto the Battlefield. It is not a Creature Card. So, it is not affected by Containment Priest.\n",
      "\n",
      "---\n",
      "\n",
      "But, if Aubrey had cast Grist, then Nickolas cast Containment Priest, then Aubrey would have been unable to put Grist onto the Battlefield. And, it would remain in Exile.\n",
      "\n",
      "---\n",
      "\n",
      "Also, if Aubrey had cast Grist, then Nickolas cast Containment Priest, then Aubrey would have been unable to put Grist onto the Battlefield. And, it would remain in Exile.\n",
      "\n",
      "---\n",
      "\n",
      "And, if Aubrey had cast Grist, then Nickolas cast Containment Priest, then Aubrey would have been unable to put Grist onto the Battlefield. And, it would remain in Exile.\n",
      "\n",
      "---\n",
      "\n",
      "And, if Aubrey had cast Grist, then Nickolas cast Containment Priest, then Aubrey would have been unable to put Grist onto the Battlefield. And, it would remain in Exile.\n",
      "\n",
      "---\n",
      "\n",
      "And, if Aubrey had cast Grist, then Nickolas cast Containment Priest, then Aubrey would have been unable to put Grist onto the Battlefield. And, it would remain in Exile.\n",
      "\n",
      "---\n",
      "\n",
      "And, if Aubrey had cast Grist, then Nickolas cast Containment Priest, then Aubrey would have been unable to put Grist onto the Battlefield. And, it would remain in Exile.\n",
      "\n",
      "---\n",
      "\n",
      "And, if Aubrey had cast Grist, then Nickolas cast Containment Priest, then Aubrey would have been unable to put Grist onto the Battlefield. And, it would remain in Exile.\n",
      "\n",
      "---\n",
      "\n",
      "And, if Aubrey had cast Grist, then Nickolas cast Containment Priest, then Aubrey would have been unable to put Grist onto the Battlefield. And, it would remain in Exile.\n",
      "\n",
      "---\n",
      "\n",
      "And, if Aubrey had cast Grist, then Nickolas cast Containment Priest, then Aubrey would have been unable to put Grist onto the Battlefield. And, it would remain in Exile.\n",
      "\n",
      "---\n",
      "\n",
      "And, if Aubrey\n"
     ]
    }
   ],
   "source": [
    "q = \"Aubrey casts Chord of Calling, and in response Nickolas casts Containment Priest. Aubrey attempts to put Grist, the Hunger Tide onto the battlefield. Will it be exiled?\"\n",
    "\n",
    "print(get_answer(q, llm_pipeline_no_peft))\n",
    "print(get_answer(q, llm_pipeline_peft))\n",
    "print(get_answer(q, llm_pipeline_peft2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 300 examples [00:00, 22133.92 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline,\n",
    ")\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer\n",
    "import wandb\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "def dataset_gen(data, allow_system_role=True):\n",
    "    for idx, row in data.iterrows():\n",
    "        if allow_system_role:\n",
    "            prompt = [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"\"\"Using the information contained in the context,\n",
    "give a comprehensive and concise answer to the question.\n",
    "Respond only to the question asked, response should be concise and relevant to the question.\n",
    "Provide the number of the rule when relevant.\n",
    "If the answer cannot be deduced from the context, do not give an answer.\n",
    "The questions are related with Magic The Gathering card game.\"\"\",\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"\"\"Context:\n",
    "{row['context']}\n",
    "---\n",
    "Now here is the question you need to answer.\n",
    "\n",
    "Question: {row['question']}\"\"\",\n",
    "                },\n",
    "                {\"role\": \"assistant\", \"content\": f\"Answer: {row['answer']}\"},\n",
    "            ]\n",
    "        else:\n",
    "            prompt = [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"\"\"Using the information contained in the context,\n",
    "give a comprehensive and concise answer to the question.\n",
    "Respond only to the question asked, response should be concise and relevant to the question.\n",
    "Provide the number of the rule when relevant.\n",
    "If the answer cannot be deduced from the context, do not give an answer.\n",
    "The questions are related with Magic The Gathering card game.\n",
    "\n",
    "Context:\n",
    "{row['context']}\n",
    "---\n",
    "Now here is the question you need to answer.\n",
    "\n",
    "Question: {row['question']}\"\"\",\n",
    "                },\n",
    "                {\"role\": \"assistant\", \"content\": f\"Answer: {row['answer']}\"},\n",
    "            ]\n",
    "\n",
    "        yield {\"messages\": prompt}\n",
    "\n",
    "\n",
    "def create_datasets(\n",
    "    data_path: str = \"data/reddit/reddit_qa_dataset_with_context.csv\",\n",
    "    num_samples: int = -1,\n",
    "    allow_system_role: bool = True,\n",
    "):\n",
    "    reddit_df = pd.read_csv(data_path)\n",
    "\n",
    "    if num_samples > 0:\n",
    "        reddit_df = reddit_df.sample(num_samples, random_state=42)\n",
    "\n",
    "    dataset = Dataset.from_generator(\n",
    "        dataset_gen,\n",
    "        gen_kwargs={\"data\": reddit_df, \"allow_system_role\": allow_system_role},\n",
    "    )\n",
    "    dataset = dataset.train_test_split(test_size=0.2)\n",
    "    train_dataset = dataset[\"train\"]\n",
    "    test_dataset = dataset[\"test\"]\n",
    "\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "\n",
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "    \"\"\"\n",
    "    Original Trainer may have a memory leak.\n",
    "    This is a workaround to avoid storing too many tensors that are not needed.\n",
    "    \"\"\"\n",
    "    pred_ids = torch.argmax(logits, dim=-1)\n",
    "    return pred_ids\n",
    "\n",
    "\n",
    "data_path: str = \"data/reddit/reddit_qa_dataset_with_context.csv\"\n",
    "\n",
    "train_dataset, test_dataset = create_datasets(\n",
    "    data_path,\n",
    "    num_samples=300,\n",
    "    allow_system_role=False,\n",
    ")\n",
    "\n",
    "# BitsAndBytesConfig int-4 config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "\n",
    "    # FIX: This is a temporary fix because there is no generation algorithm\n",
    "    # So we just return the argmax of the logits\n",
    "    # There is a trainer parameter called: `preprocess_logits_for_metrics`\n",
    "    # preds = preds.argmax(-1)\n",
    "\n",
    "    # decode preds and labels\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    preds = np.where(preds != -100, preds, tokenizer.pad_token_id)\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    scores = rouge.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=decoded_labels,\n",
    "        rouge_types=[\"rouge1\", \"rouge2\", \"rougeL\"],\n",
    "        use_aggregator=True,\n",
    "        use_stemmer=True,\n",
    "    )\n",
    "\n",
    "    scores[\"bleu\"] = bleu.compute(predictions=decoded_preds, references=decoded_labels)[\n",
    "        \"bleu\"\n",
    "    ]\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "# Training Params\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"output\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    eval_accumulation_steps=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    logging_steps=1,\n",
    "    eval_steps=50,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    learning_rate=5e-5,\n",
    "    bf16=True,\n",
    "    tf32=True,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    report_to=\"wandb\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"loss\",\n",
    "    greater_is_better=False,\n",
    "    save_total_limit=1,  # Save only the most recent checkpoint\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "max_seq_length = 3072  # max sequence length for model and packing of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  1.83it/s]\n",
      "Map: 100%|██████████| 240/240 [00:00<00:00, 2563.86 examples/s]\n",
      "Map: 100%|██████████| 60/60 [00:00<00:00, 2718.34 examples/s]\n",
      "/home/javierj/miniconda3/envs/gatherer-sage/lib/python3.9/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "base_model_path = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "adapter_path = \"model/mistral-gatherer-sage-v1/4vgx59fr/best_model\"\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_path,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "\n",
    "model_peft = PeftModel.from_pretrained(model, adapter_path)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model_peft,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    # packing=True,\n",
    "    preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    "    dataset_kwargs={\n",
    "        \"add_special_tokens\": False,  # We template with special tokens\n",
    "        \"append_concat_token\": False,  # No need to add additional separator token\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 00:32]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjavier-jimenez99\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/javierj/gatherer-sage/wandb/run-20240603_145319-9nj9qog5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/javier-jimenez99/huggingface/runs/9nj9qog5' target=\"_blank\">output</a></strong> to <a href='https://wandb.ai/javier-jimenez99/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/javier-jimenez99/huggingface' target=\"_blank\">https://wandb.ai/javier-jimenez99/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/javier-jimenez99/huggingface/runs/9nj9qog5' target=\"_blank\">https://wandb.ai/javier-jimenez99/huggingface/runs/9nj9qog5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.29772478342056274,\n",
       " 'eval_rouge1': 0.9274727458822571,\n",
       " 'eval_rouge2': 0.8332162826848648,\n",
       " 'eval_rougeL': 0.8994541068093473,\n",
       " 'eval_bleu': 0.8269862546802125,\n",
       " 'eval_runtime': 47.2802,\n",
       " 'eval_samples_per_second': 1.269,\n",
       " 'eval_steps_per_second': 0.635}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Target module Dropout(p=0.05, inplace=False) is not supported. Currently, only the following modules are supported: `torch.nn.Linear`, `torch.nn.Embedding`, `torch.nn.Conv2d`, `transformers.pytorch_utils.Conv1D`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m peft_config \u001b[38;5;241m=\u001b[39m LoraConfig(\n\u001b[1;32m      2\u001b[0m     lora_alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.5\u001b[39m,\n\u001b[1;32m      3\u001b[0m     lora_dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m     task_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCAUSAL_LM\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      8\u001b[0m )\n\u001b[0;32m---> 10\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mSFTTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# packing=True,\u001b[39;49;00m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreprocess_logits_for_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreprocess_logits_for_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43madd_special_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# We template with special tokens\u001b[39;49;00m\n\u001b[1;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mappend_concat_token\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# No need to add additional separator token\u001b[39;49;00m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m trainer\u001b[38;5;241m.\u001b[39mevaluate()\n",
      "File \u001b[0;32m~/miniconda3/envs/gatherer-sage/lib/python3.9/site-packages/trl/trainer/sft_trainer.py:228\u001b[0m, in \u001b[0;36mSFTTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics, peft_config, dataset_text_field, packing, formatting_func, max_seq_length, infinite, num_of_sequences, chars_per_token, dataset_num_proc, dataset_batch_size, neftune_noise_alpha, model_init_kwargs, dataset_kwargs, eval_packing)\u001b[0m\n\u001b[1;32m    224\u001b[0m             output\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    226\u001b[0m         model\u001b[38;5;241m.\u001b[39mget_input_embeddings()\u001b[38;5;241m.\u001b[39mregister_forward_hook(make_inputs_require_grad)\n\u001b[0;32m--> 228\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mget_peft_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    230\u001b[0m     args \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m args\u001b[38;5;241m.\u001b[39mbf16\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_loaded_in_4bit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sharded_qlora\n\u001b[1;32m    234\u001b[0m ):\n\u001b[1;32m    235\u001b[0m     peft_module_casting_to_bf16(model)\n",
      "File \u001b[0;32m~/miniconda3/envs/gatherer-sage/lib/python3.9/site-packages/peft/mapping.py:149\u001b[0m, in \u001b[0;36mget_peft_model\u001b[0;34m(model, peft_config, adapter_name, mixed)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m peft_config\u001b[38;5;241m.\u001b[39mis_prompt_learning:\n\u001b[1;32m    148\u001b[0m     peft_config \u001b[38;5;241m=\u001b[39m _prepare_prompt_learning_config(peft_config, model_config)\n\u001b[0;32m--> 149\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMODEL_TYPE_TO_PEFT_MODEL_MAPPING\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gatherer-sage/lib/python3.9/site-packages/peft/peft_model.py:1395\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.__init__\u001b[0;34m(self, model, peft_config, adapter_name)\u001b[0m\n\u001b[1;32m   1394\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model: torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule, peft_config: PeftConfig, adapter_name: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1395\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1396\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model_prepare_inputs_for_generation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation\n",
      "File \u001b[0;32m~/miniconda3/envs/gatherer-sage/lib/python3.9/site-packages/peft/peft_model.py:138\u001b[0m, in \u001b[0;36mPeftModel.__init__\u001b[0;34m(self, model, peft_config, adapter_name)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_peft_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m PEFT_TYPE_TO_MODEL_MAPPING[peft_config\u001b[38;5;241m.\u001b[39mpeft_type]\n\u001b[0;32m--> 138\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_additional_trainable_modules(peft_config, adapter_name)\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_gradient_checkpointing\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m~/miniconda3/envs/gatherer-sage/lib/python3.9/site-packages/peft/tuners/lora/model.py:139\u001b[0m, in \u001b[0;36mLoraModel.__init__\u001b[0;34m(self, model, config, adapter_name)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, config, adapter_name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 139\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gatherer-sage/lib/python3.9/site-packages/peft/tuners/tuners_utils.py:166\u001b[0m, in \u001b[0;36mBaseTuner.__init__\u001b[0;34m(self, model, peft_config, adapter_name)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive_adapter: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m adapter_name\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pre_injection_hook(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_config[adapter_name], adapter_name)\n\u001b[0;32m--> 166\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minject_adapter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;66;03m# Copy the peft_config in the injected model.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpeft_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_config\n",
      "File \u001b[0;32m~/miniconda3/envs/gatherer-sage/lib/python3.9/site-packages/peft/tuners/tuners_utils.py:372\u001b[0m, in \u001b[0;36mBaseTuner.inject_adapter\u001b[0;34m(self, model, adapter_name)\u001b[0m\n\u001b[1;32m    370\u001b[0m     is_target_modules_in_base_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    371\u001b[0m     parent, target, target_name \u001b[38;5;241m=\u001b[39m _get_submodules(model, key)\n\u001b[0;32m--> 372\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_and_replace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_target_modules_in_base_model:\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    376\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget modules \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpeft_config\u001b[38;5;241m.\u001b[39mtarget_modules\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in the base model. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    377\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease check the target modules and try again.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    378\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/gatherer-sage/lib/python3.9/site-packages/peft/tuners/lora/model.py:223\u001b[0m, in \u001b[0;36mLoraModel._create_and_replace\u001b[0;34m(self, lora_config, adapter_name, target, target_name, parent, current_key)\u001b[0m\n\u001b[1;32m    213\u001b[0m     target\u001b[38;5;241m.\u001b[39mupdate_layer(\n\u001b[1;32m    214\u001b[0m         adapter_name,\n\u001b[1;32m    215\u001b[0m         r,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    220\u001b[0m         use_dora\u001b[38;5;241m=\u001b[39mlora_config\u001b[38;5;241m.\u001b[39muse_dora,\n\u001b[1;32m    221\u001b[0m     )\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 223\u001b[0m     new_module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlora_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m adapter_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive_adapters:\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;66;03m# adding an additional adapter: it is not automatically trainable\u001b[39;00m\n\u001b[1;32m    226\u001b[0m         new_module\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/gatherer-sage/lib/python3.9/site-packages/peft/tuners/lora/model.py:320\u001b[0m, in \u001b[0;36mLoraModel._create_new_module\u001b[0;34m(lora_config, adapter_name, target, **kwargs)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;66;03m# no module could be matched\u001b[39;00m\n\u001b[0;32m--> 320\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget module \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not supported. Currently, only the following modules are supported: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`torch.nn.Linear`, `torch.nn.Embedding`, `torch.nn.Conv2d`, `transformers.pytorch_utils.Conv1D`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    323\u001b[0m     )\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_module\n",
      "\u001b[0;31mValueError\u001b[0m: Target module Dropout(p=0.05, inplace=False) is not supported. Currently, only the following modules are supported: `torch.nn.Linear`, `torch.nn.Embedding`, `torch.nn.Conv2d`, `transformers.pytorch_utils.Conv1D`."
     ]
    }
   ],
   "source": [
    "peft_config = LoraConfig(\n",
    "    lora_alpha=256 * 0.5,\n",
    "    lora_dropout=0.05,\n",
    "    r=256,\n",
    "    bias=\"none\",\n",
    "    target_modules=\"all-linear\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    peft_config=peft_config,\n",
    "    eval_dataset=test_dataset,\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    # packing=True,\n",
    "    preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    "    dataset_kwargs={\n",
    "        \"add_special_tokens\": False,  # We template with special tokens\n",
    "        \"append_concat_token\": False,  # No need to add additional separator token\n",
    "    },\n",
    ")\n",
    "trainer.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gatherer-sage",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
