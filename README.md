# gatherer-sage
AI chatbot to answer MTG ruling questions

## Sources
### Datasets
- MTG rules: [https://media.wizards.com/2024/downloads/MagicCompRules%2004102024.txt](https://media.wizards.com/2024/downloads/MagicCompRules%2004102024.txt)
- Ruling Subreddit: [https://www.reddit.com/r/mtgrules/](https://www.reddit.com/r/mtgrules/)
- MTG Salvation Forum: [https://www.mtgsalvation.com/forums/magic-fundamentals/magic-rulings](https://www.mtgsalvation.com/forums/magic-fundamentals/magic-rulings)
- Rules Q&A:[https://rulesguru.net/](https://rulesguru.net/)

### Data Utils
- Download Subreddit: [https://pushshift.io/signup](https://pushshift.io/signup)
- Hyperlink MTG Rules: [https://yawgatog.com/resources/magic-rules/](https://yawgatog.com/resources/magic-rules/)

### Models
- Llama 3: [8B-instruct](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct) and [70B-instruct](https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct)
- Mistral: [7B-instruct](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3)

### Models Utils
- VRAM requirements: [https://www.reddit.com/r/LocalLLaMA/comments/18o5u0k/helpful_vram_requirement_table_for_qlora_lora_and/](https://www.reddit.com/r/LocalLLaMA/comments/18o5u0k/helpful_vram_requirement_table_for_qlora_lora_and/)
- QLora example: [https://dassum.medium.com/fine-tune-large-language-model-llm-on-a-custom-dataset-with-qlora-fb60abdeba07](https://dassum.medium.com/fine-tune-large-language-model-llm-on-a-custom-dataset-with-qlora-fb60abdeba07)
- Fine-tuning example: [https://github.com/huggingface/blog/blob/main/Lora-for-sequence-classification-with-Roberta-Llama-Mistral.md](https://github.com/huggingface/blog/blob/main/Lora-for-sequence-classification-with-Roberta-Llama-Mistral.md)
