{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from gatherer_sage.rag import RAG\n",
    "\n",
    "PROMPT_TEMPLATE = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"\"\"Using the information contained in the context,\n",
    "give a comprehensive and concise answer to the question.\n",
    "Respond only to the question asked, response should be concise and relevant to the question.\n",
    "Provide the number of the rule when relevant.\n",
    "If the answer cannot be deduced from the context, do not give an answer.\n",
    "The questions are related with Magic The Gathering card game.\"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"\"\"Context:\n",
    "{context}\n",
    "---\n",
    "Now here is the question you need to answer.\n",
    "\n",
    "Question: {question}\"\"\",\n",
    "    },\n",
    "]\n",
    "\n",
    "READER_MODEL_NAME = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "\n",
    "class RedditDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        reddit_data: pd.DataFrame,\n",
    "        rag,\n",
    "        prompt_template=PROMPT_TEMPLATE,\n",
    "        model_path=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    ):\n",
    "        self.rag = rag\n",
    "        self.data = reddit_data\n",
    "        self.prompt_template = prompt_template\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        self.prompt_template = self.tokenizer.apply_chat_template(\n",
    "            prompt_template, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        question = row[\"question\"]\n",
    "        context = self.rag.retrieve_context(question)\n",
    "        complete_prompt = self.prompt_template.format(\n",
    "            question=question, context=context\n",
    "        )\n",
    "        return complete_prompt\n",
    "\n",
    "\n",
    "rag = RAG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    READER_MODEL_NAME, quantization_config=bnb_config\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(READER_MODEL_NAME)\n",
    "\n",
    "READER_LLM = pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    do_sample=True,\n",
    "    temperature=0.2,\n",
    "    repetition_penalty=1.1,\n",
    "    return_full_text=False,\n",
    "    max_new_tokens=500,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gatherer-sage",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
